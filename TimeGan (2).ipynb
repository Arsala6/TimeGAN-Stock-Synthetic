{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# âœ… Path update karo agar file location alag ho\n",
        "path = \"/content/drive/MyDrive/all_stocks_5yr.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(\"âœ… File loaded successfully!\")\n",
        "print(df.columns)\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.set_index(\"date\")\n",
        "df = df.interpolate(method=\"time\").bfill().ffill()\n",
        "\n",
        "features = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "df = df[features]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(df)\n",
        "\n",
        "def create_sequences(data, seq_len=24):\n",
        "    seqs = []\n",
        "    for i in range(len(data) - seq_len):\n",
        "        seqs.append(data[i:i+seq_len])\n",
        "    return np.array(seqs)\n",
        "\n",
        "sequences = create_sequences(scaled, 24)\n",
        "X_train = sequences[:int(0.8*len(sequences))]\n",
        "\n",
        "print(\"âœ… Sequences shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "5oKWbrNUGkmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class SimpleTimeGAN(tf.keras.Model):\n",
        "    def __init__(self, timesteps, features, hidden_dim=24, num_layers=3):\n",
        "        super(SimpleTimeGAN, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.features = features\n",
        "\n",
        "        self.encoder = tf.keras.Sequential(\n",
        "            [tf.keras.layers.LSTM(hidden_dim, return_sequences=True)] +\n",
        "            [tf.keras.layers.LSTM(hidden_dim) for _ in range(num_layers - 1)]\n",
        "        )\n",
        "        self.generator = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=True),\n",
        "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(features, activation='sigmoid'))\n",
        "        ])\n",
        "        self.discriminator = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=False),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def compile(self, g_opt, d_opt):\n",
        "        super(SimpleTimeGAN, self).compile()\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.bce = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_seq):\n",
        "        batch_size = tf.shape(real_seq)[0]\n",
        "        noise = tf.random.normal((batch_size, self.timesteps, self.features))\n",
        "\n",
        "        # Train discriminator\n",
        "        with tf.GradientTape() as tape_d:\n",
        "            fake_seq = self.generator(noise)\n",
        "            real_pred = self.discriminator(real_seq)\n",
        "            fake_pred = self.discriminator(fake_seq)\n",
        "            d_loss = self.bce(tf.ones_like(real_pred), real_pred) + \\\n",
        "                     self.bce(tf.zeros_like(fake_pred), fake_pred)\n",
        "        grads_d = tape_d.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(grads_d, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Train generator\n",
        "        with tf.GradientTape() as tape_g:\n",
        "            fake_seq = self.generator(noise)\n",
        "            fake_pred = self.discriminator(fake_seq)\n",
        "            g_loss = self.bce(tf.ones_like(fake_pred), fake_pred)\n",
        "        grads_g = tape_g.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(grads_g, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "timesteps = X_train.shape[1]\n",
        "features = X_train.shape[2]\n",
        "\n",
        "model = SimpleTimeGAN(timesteps, features)\n",
        "model.compile(\n",
        "    g_opt=tf.keras.optimizers.Adam(1e-4),\n",
        "    d_opt=tf.keras.optimizers.Adam(1e-4)\n",
        ")\n",
        "\n",
        "history = []\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i in range(0, len(X_train), BATCH_SIZE):\n",
        "        batch = X_train[i:i+BATCH_SIZE]\n",
        "        metrics = model.train_step(batch)\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch}: D_loss={metrics['d_loss']:.4f}, G_loss={metrics['g_loss']:.4f}\")\n",
        "noise = tf.random.normal((1, timesteps, features))\n",
        "synthetic_data = model.generator(noise).numpy()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(X_train[0,:,0], label=\"Real (Open)\")\n",
        "plt.plot(synthetic_data[0,:,0], '--', label=\"Synthetic (Open)\")\n",
        "plt.title(\"Real vs Synthetic (Open Feature)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mmw8Xv0cG0nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# âœ… Step 1: Generate synthetic samples\n",
        "num_samples = 1000\n",
        "noise = tf.random.normal((num_samples, timesteps, features))\n",
        "synthetic_data = model.generator(noise).numpy()\n",
        "\n",
        "# Flatten for visualization\n",
        "real_flat = X_train[:num_samples].reshape(num_samples, -1)\n",
        "synthetic_flat = synthetic_data.reshape(num_samples, -1)\n",
        "\n",
        "# âœ… Step 2: PCA (for dimensionality reduction)\n",
        "pca = PCA(n_components=2)\n",
        "pca_real = pca.fit_transform(real_flat)\n",
        "pca_synth = pca.transform(synthetic_flat)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_real[:,0], pca_real[:,1], alpha=0.5, label='Real', s=10)\n",
        "plt.scatter(pca_synth[:,0], pca_synth[:,1], alpha=0.5, label='Synthetic', s=10)\n",
        "plt.title(\"PCA Comparison: Real vs Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# âœ… Step 3: t-SNE visualization (for better structure)\n",
        "combined = np.vstack([real_flat, synthetic_flat])\n",
        "labels = np.array([0]*num_samples + [1]*num_samples)\n",
        "tsne = TSNE(n_components=2, perplexity=40, learning_rate=200, n_iter=1000)\n",
        "tsne_results = tsne.fit_transform(combined)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(tsne_results[labels==0,0], tsne_results[labels==0,1], label=\"Real\", s=10, alpha=0.6)\n",
        "plt.scatter(tsne_results[labels==1,0], tsne_results[labels==1,1], label=\"Synthetic\", s=10, alpha=0.6)\n",
        "plt.title(\"t-SNE Comparison: Real vs Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# âœ… Step 4: Save synthetic data to CSV\n",
        "synthetic_df = pd.DataFrame(\n",
        "    synthetic_data.reshape(-1, features),\n",
        "    columns=[f\"feature_{i+1}\" for i in range(features)]\n",
        ")\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/synthetic_timegan_data.csv\"\n",
        "synthetic_df.to_csv(csv_path, index=False)\n",
        "print(f\"âœ… Synthetic data saved successfully at: {csv_path}\")\n"
      ],
      "metadata": {
        "id": "jraWMFqXirpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# âœ… Step 1: Generate synthetic samples\n",
        "num_samples = 1000\n",
        "noise = tf.random.normal((num_samples, timesteps, features))\n",
        "synthetic_data = model.generator(noise).numpy()\n",
        "\n",
        "# âœ… Step 2: Flatten data for visualization\n",
        "real_flat = X_train[:num_samples].reshape(num_samples, -1)\n",
        "synthetic_flat = synthetic_data.reshape(num_samples, -1)\n",
        "\n",
        "# âœ… Step 3: PCA (for 2D comparison)\n",
        "pca = PCA(n_components=2)\n",
        "pca_real = pca.fit_transform(real_flat)\n",
        "pca_synth = pca.transform(synthetic_flat)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_real[:,0], pca_real[:,1], alpha=0.5, label='Real', s=10)\n",
        "plt.scatter(pca_synth[:,0], pca_synth[:,1], alpha=0.5, label='Synthetic', s=10)\n",
        "plt.title(\"PCA Comparison: Real vs Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# âœ… Step 4: t-SNE visualization (for detailed structure)\n",
        "combined = np.vstack([real_flat, synthetic_flat])\n",
        "labels = np.array([0]*num_samples + [1]*num_samples)\n",
        "tsne = TSNE(n_components=2, perplexity=40, learning_rate=200, n_iter=1000)\n",
        "tsne_results = tsne.fit_transform(combined)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(tsne_results[labels==0,0], tsne_results[labels==0,1], label=\"Real\", s=10, alpha=0.6)\n",
        "plt.scatter(tsne_results[labels==1,0], tsne_results[labels==1,1], label=\"Synthetic\", s=10, alpha=0.6)\n",
        "plt.title(\"t-SNE Comparison: Real vs Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# âœ… Step 5: Save synthetic data with feature names\n",
        "feature_names = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "\n",
        "synthetic_df = pd.DataFrame(\n",
        "    synthetic_data.reshape(-1, features),\n",
        "    columns=feature_names\n",
        ")\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/synthetic_timegan_data_named.csv\"\n",
        "synthetic_df.to_csv(csv_path, index=False)\n",
        "print(f\"âœ… Synthetic data saved successfully at:\\nğŸ“ {csv_path}\")\n",
        "\n",
        "# âœ… Preview first few rows\n",
        "print(\"\\nğŸ”¹ Sample synthetic data preview:\")\n",
        "print(synthetic_df.head())\n"
      ],
      "metadata": {
        "id": "ieqsN7UKlLGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtaidistance\n"
      ],
      "metadata": {
        "id": "inaIDSPiW3YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance import dtw\n",
        "print(\"âœ… DTW module imported successfully!\")\n"
      ],
      "metadata": {
        "id": "xPKybwvkXRhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 2. Load Dataset from Drive\n",
        "path = \"/content/drive/MyDrive/all_stocks_5yr.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(\"âœ… File loaded successfully!\")\n",
        "print(df.head())\n",
        "\n",
        "# 3. Preprocessing\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])      # Convert to datetime\n",
        "df = df.set_index(\"date\")                    # âœ… Set datetime index for time interpolation\n",
        "\n",
        "# Handle missing values (interpolate + backfill)\n",
        "df = df.interpolate(method=\"time\").bfill().ffill()\n",
        "print(\"âœ… Missing values handled.\")\n",
        "\n",
        "# Select useful numeric columns\n",
        "features = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "df = df[features]\n",
        "print(\"âœ… Feature selection complete.\")\n",
        "print(df.head())\n",
        "\n",
        "# Normalize (0â€“1 scaling)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "print(\"âœ… Data scaled between 0â€“1\")\n",
        "\n",
        "# Create sequences (e.g., 24 timesteps)\n",
        "def create_sequences(data, timesteps=24):\n",
        "    seqs = []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        seqs.append(data[i:i+timesteps])\n",
        "    return np.array(seqs)\n",
        "\n",
        "timesteps = 24\n",
        "X_train = create_sequences(scaled_data, timesteps)\n",
        "print(\"âœ… Sequences created:\", X_train.shape)\n",
        "\n",
        "# 4. Simple TimeGAN Model\n",
        "class SimpleTimeGAN(tf.keras.Model):\n",
        "    def __init__(self, timesteps, features, hidden_dim=24, num_layers=3):\n",
        "        super(SimpleTimeGAN, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.features = features\n",
        "\n",
        "        self.encoder = tf.keras.Sequential(\n",
        "            [tf.keras.layers.LSTM(hidden_dim, return_sequences=True)] +\n",
        "            [tf.keras.layers.LSTM(hidden_dim) for _ in range(num_layers - 1)]\n",
        "        )\n",
        "        self.generator = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=True),\n",
        "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(features, activation='sigmoid'))\n",
        "        ])\n",
        "        self.discriminator = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=False),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def compile(self, g_opt, d_opt):\n",
        "        super(SimpleTimeGAN, self).compile()\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.bce = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_seq):\n",
        "        batch_size = tf.shape(real_seq)[0]\n",
        "        noise = tf.random.normal((batch_size, self.timesteps, self.features))\n",
        "\n",
        "        # --- Train Discriminator ---\n",
        "        with tf.GradientTape() as tape_d:\n",
        "            fake_seq = self.generator(noise)\n",
        "            real_pred = self.discriminator(real_seq)\n",
        "            fake_pred = self.discriminator(fake_seq)\n",
        "            d_loss = self.bce(tf.ones_like(real_pred), real_pred) + \\\n",
        "                     self.bce(tf.zeros_like(fake_pred), fake_pred)\n",
        "        grads_d = tape_d.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(grads_d, self.discriminator.trainable_variables))\n",
        "\n",
        "        # --- Train Generator ---\n",
        "        with tf.GradientTape() as tape_g:\n",
        "            fake_seq = self.generator(noise)\n",
        "            fake_pred = self.discriminator(fake_seq)\n",
        "            g_loss = self.bce(tf.ones_like(fake_pred), fake_pred)\n",
        "        grads_g = tape_g.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(grads_g, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n",
        "\n",
        "# 5. Initialize Model\n",
        "timesteps = X_train.shape[1]\n",
        "features = X_train.shape[2]\n",
        "\n",
        "model = SimpleTimeGAN(timesteps, features)\n",
        "model.compile(\n",
        "    g_opt=tf.keras.optimizers.Adam(1e-4),\n",
        "    d_opt=tf.keras.optimizers.Adam(1e-4)\n",
        ")\n",
        "\n",
        "# 6. Train Model\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "history = []\n",
        "\n",
        "print(\"ğŸ‹ï¸â€â™‚ï¸ Training started... (this will take a few minutes)\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i in range(0, len(X_train), BATCH_SIZE):\n",
        "        batch = X_train[i:i+BATCH_SIZE]\n",
        "        metrics = model.train_step(batch)\n",
        "    history.append(metrics)\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:03d}: D_loss={metrics['d_loss']:.4f}, G_loss={metrics['g_loss']:.4f}\")\n",
        "\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# 7. Generate Synthetic Data\n",
        "noise = tf.random.normal((1, timesteps, features))\n",
        "synthetic_data = model.generator(noise).numpy()\n",
        "\n",
        "# 8. Visualization: Real vs Synthetic\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(X_train[0,:,0], label=\"Real (Open)\")\n",
        "plt.plot(synthetic_data[0,:,0], '--', label=\"Synthetic (Open)\")\n",
        "plt.title(\"ğŸ“ˆ Real vs Synthetic Stock Data (Open Price)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G4L0KPTzmDOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ğŸ“Š 9. Visualization & Analytics Section\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ---------- Create Synthetic Samples ----------\n",
        "noise = tf.random.normal((100, X_train.shape[1], X_train.shape[2]))\n",
        "synthetic_data = model.generator(noise).numpy()\n",
        "\n",
        "# âœ… Match real and synthetic sample sizes\n",
        "n_samples = min(len(X_train), len(synthetic_data))\n",
        "real_subset = X_train[:n_samples]\n",
        "synth_subset = synthetic_data[:n_samples]\n",
        "\n",
        "# âœ… Define feature names\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "\n",
        "# ---------- (a) Real vs Synthetic Overlay ----------\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(real_subset[0,:,0], label=\"Real (Open)\")\n",
        "plt.plot(synth_subset[0,:,0], '--', label=\"Synthetic (Open)\")\n",
        "plt.title(\"ğŸ“ˆ Real vs Synthetic Stock Data (Open)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------- (b) Distribution Comparison ----------\n",
        "real_flat = real_subset.reshape(-1, X_train.shape[2])\n",
        "synth_flat = synth_subset.reshape(-1, X_train.shape[2])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, feature in enumerate(features):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    sns.kdeplot(real_flat[:, i], label=\"Real\", fill=True)\n",
        "    sns.kdeplot(synth_flat[:, i], label=\"Synthetic\", fill=True, linestyle=\"--\")\n",
        "    plt.title(feature)\n",
        "plt.suptitle(\"ğŸ“Š Distribution Comparison: Real vs Synthetic Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- (c) Correlation Heatmaps ----------\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.heatmap(pd.DataFrame(real_flat, columns=features).corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Real Correlation\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.heatmap(pd.DataFrame(synth_flat, columns=features).corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Synthetic Correlation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- (d) Dynamic Time Warping (DTW) ----------\n",
        "!pip install fastdtw > /dev/null\n",
        "from fastdtw import fastdtw\n",
        "dtw_distances = []\n",
        "for i in range(50):\n",
        "    d, _ = fastdtw(real_subset[i,:,0], synth_subset[i,:,0])\n",
        "    dtw_distances.append(d)\n",
        "avg_dtw = np.mean(dtw_distances)\n",
        "print(f\"ğŸ“ Average DTW Distance (lower = better): {avg_dtw:.4f}\")\n",
        "\n",
        "# ---------- (e) Maximum Mean Discrepancy (MMD) ----------\n",
        "def compute_mmd(x, y, gamma=1.0):\n",
        "    xx = rbf_kernel(x, x, gamma)\n",
        "    yy = rbf_kernel(y, y, gamma)\n",
        "    xy = rbf_kernel(x, y, gamma)\n",
        "    return xx.mean() + yy.mean() - 2 * xy.mean()\n",
        "\n",
        "mmd_score = compute_mmd(real_flat, synth_flat)\n",
        "print(f\"ğŸ§® MMD Score (lower = better): {mmd_score:.6f}\")\n",
        "\n",
        "# ---------- (f) Predictive Utility ----------\n",
        "X_real, y_real = real_subset[:,:-1,:], real_subset[:,-1,0]\n",
        "X_syn, y_syn = synth_subset[:,:-1,:], synth_subset[:,-1,0]\n",
        "\n",
        "X_real = X_real.reshape(len(X_real), -1)\n",
        "X_syn = X_syn.reshape(len(X_syn), -1)\n",
        "\n",
        "model_lr = LinearRegression().fit(X_syn, y_syn)\n",
        "y_pred = model_lr.predict(X_real)\n",
        "mse = mean_squared_error(y_real, y_pred)\n",
        "r2 = r2_score(y_real, y_pred)\n",
        "print(f\"ğŸ“‰ Predictive Utility â€” MSE: {mse:.6f}, RÂ²: {r2:.4f}\")\n",
        "\n",
        "# ğŸ“‹ 10. Summary Table\n",
        "summary = pd.DataFrame({\n",
        "    \"Metric\": [\"DTW Distance\", \"MMD Score\", \"Predictive MSE\", \"Predictive RÂ²\"],\n",
        "    \"Value\": [avg_dtw, mmd_score, mse, r2],\n",
        "    \"Interpretation\": [\n",
        "        \"Measures shape similarity of temporal patterns\",\n",
        "        \"Measures distribution similarity between real & synthetic data\",\n",
        "        \"Tests generalization from syntheticâ†’real forecasting\",\n",
        "        \"Higher = better predictive realism\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nğŸ“Š Quantitative Evaluation Summary\")\n",
        "display(summary)\n"
      ],
      "metadata": {
        "id": "7BEtC4Vl3QBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}